{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "822b57de-71dc-4818-a8ed-f2871c1b7d70",
   "metadata": {},
   "source": [
    "# Text Generation using LSTM (Word-Level)\n",
    "\n",
    "This mini-project demonstrates how to build a simple **word-level text generation model** using **LSTM (Long Short-Term Memory)** neural networks.\n",
    "\n",
    "We use a small dummy dataset consisting of simple English sentences. The goal is to train an LSTM model to predict the next word in a sequence based on the previous words.\n",
    "\n",
    "### Project Goals\n",
    "- Tokenize and preprocess text data at the word level\n",
    "- Prepare training sequences suitable for LSTM\n",
    "- Build and train an LSTM-based language model\n",
    "- Generate new text given a starting seed\n",
    "- Understand key NLP concepts like:\n",
    "  - Word tokenization\n",
    "  - Sequence generation\n",
    "  - Padding\n",
    "  - LSTM and dense layers\n",
    "\n",
    "### Tools & Libraries\n",
    "- Python\n",
    "- TensorFlow / Keras\n",
    "- NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e0bdee0-2166-41b3-9d27-7ec6aff5e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e470d-408a-4207-9064-9fa025fecb86",
   "metadata": {},
   "source": [
    "### Step 1 â€“ Dataset Preparation\n",
    "\n",
    "##### Load the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc1ab55f-b598-4300-a692-4981e69e7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "\n",
    "# Dummy text corpus\n",
    "sample_text = \"\"\"Once upon a time in a quiet village, there lived a curious girl named Lily. \n",
    "Every morning, she wandered through the fields, chasing butterflies and collecting colorful stones. \n",
    "She believed every stone had a story, a secret it whispered only to her. \n",
    "One day, she found a glowing pebble near the old oak tree. \n",
    "From that moment, strange dreams visited her, showing her distant lands and talking animals. \n",
    "Lily knew it was the beginning of an adventure. She packed her bag, hugged her family, and stepped into the forest, following the silent song only she could hear.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b40de9-158b-44ea-b6c4-7f59065a3af1",
   "metadata": {},
   "source": [
    "### Step 2: Text Cleaning and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2bb63241-e4c5-4bf1-b13f-4d657a39cb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:  97\n",
      "First 50 Tokens:  ['once', 'upon', 'a', 'time', 'in', 'a', 'quiet', 'village', 'there', 'lived', 'a', 'curious', 'girl', 'named', 'lily', 'every', 'morning', 'she', 'wandered', 'through', 'the', 'fields', 'chasing', 'butterflies', 'and', 'collecting', 'colorful', 'stones', 'she', 'believed', 'every', 'stone', 'had', 'a', 'story', 'a', 'secret', 'it', 'whispered', 'only', 'to', 'her', 'one', 'day', 'she', 'found', 'a', 'glowing', 'pebble', 'near']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Lowercase\n",
    "text = sample_text.lower()\n",
    "\n",
    "# # Remove unwanted characters (keep letters and basic punctuation)\n",
    "text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Total Tokens: \", len(tokens))\n",
    "print(\"First 50 Tokens: \", tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635bb68f-0e4d-41bb-b1f5-24a415620f21",
   "metadata": {},
   "source": [
    "### Step 3: Sequence Creation for Training\n",
    "\n",
    "Now that we have a cleaned list of words `(tokens)`, we'll prepare training sequences.\n",
    "\n",
    "Goal:\n",
    "* We want to create many input sequences where each sequence is a few words long, and the next word is the **label**.\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "Input Sequence: [\"alice\", \"was\", \"beginning\", \"to\"]    \n",
    "\n",
    "Label: \"get\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0944171b-b124-46c2-ad0e-c6fb4cd91f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dda17eba-512e-4b1e-88cd-d6b4ea31285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences size:  97\n",
      "Sequence:  [10, 11, 1, 12, 13, 1, 14, 15, 16, 17]\n",
      "\n",
      "Vocabulary Size:  75\n",
      "\n",
      "first 5 input sequences:\n",
      " [[10, 11], [10, 11, 1], [10, 11, 1, 12], [10, 11, 1, 12, 13], [10, 11, 1, 12, 13, 1]]\n",
      "\n",
      "first 5 input sequences After padding:\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10\n",
      "  11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 11\n",
      "   1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 11  1\n",
      "  12]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 11  1 12\n",
      "  13]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 11  1 12 13\n",
      "   1]]\n",
      "\n",
      "Input features type:  <class 'numpy.ndarray'>\n",
      "Label type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and fit on text\n",
    "\n",
    "\"\"\"Below step builds the vocabulary from the list of tokens (words). \n",
    "   Each unique word gets assigned an integer index. \n",
    "   We're using those integer indexes as sequences values. Ex: \"alices\" word have index 298 which we're using to represent that word.\n",
    "\"\"\"\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokens)      # Learn the vocabulary from the text\n",
    "\n",
    "# Convert words to integers\n",
    "sequences = tokenizer.texts_to_sequences([tokens])[0]\n",
    "print(\"sequences size: \", len(sequences))\n",
    "print(\"Sequence: \", sequences[:10])\n",
    "\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"\\nVocabulary Size: \", vocab_size)\n",
    "\n",
    "\n",
    "# Create input sequences and labels\n",
    "input_sequences = []\n",
    "for i in range(1, len(sequences)):\n",
    "    input_sequences.append(sequences[:i+1])\n",
    "print(\"\\nfirst 5 input sequences:\\n\", input_sequences[:5])\n",
    "\n",
    "\n",
    "\n",
    "# Pad sequences to same length\n",
    "max_seq_len = len(input_sequences[-1])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "print(\"\\nfirst 5 input sequences After padding:\\n\", input_sequences[:5])\n",
    "\n",
    "# Split into input (X) and label (y)\n",
    "input_sequences = np.array(input_sequences)\n",
    "\"\"\"Below code is used to get values from 2D array\"\"\"\n",
    "x = input_sequences[:, :-1]  # All rows and all columns expect last one\n",
    "y = input_sequences[:, -1]   # All rows and last column\n",
    "\n",
    "print(\"\\nInput features type: \", type(x))\n",
    "print(\"Label type: \", type(y))\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326825ae-85f2-4e38-838e-e18d94ababe6",
   "metadata": {},
   "source": [
    "#### Step 4: Building the LSTM Text Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "376dd461-4d33-4441-8d3d-ec4d3a992c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 96, 64)            4800      \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 100)               66000     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 75)                7575      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,375\n",
      "Trainable params: 78,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:51:28.632938: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-06-01 10:51:28.633830: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-06-01 10:51:28.634908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=max_seq_len - 1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2ded279c-22e3-453a-bf2b-6d92a0e20270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:51:29.125466: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-06-01 10:51:29.127415: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-06-01 10:51:29.128810: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-06-01 10:51:29.591147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-06-01 10:51:29.592592: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-06-01 10:51:29.593690: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 105ms/step - loss: 4.3182 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 4.3089 - accuracy: 0.0729\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 4.3003 - accuracy: 0.1146\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 4.2900 - accuracy: 0.1354\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 4.2768 - accuracy: 0.1146\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 4.2476 - accuracy: 0.0625\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 4.1845 - accuracy: 0.0625\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 4.1352 - accuracy: 0.0625\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 4.1022 - accuracy: 0.0938\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 4.0708 - accuracy: 0.0729\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 4.0285 - accuracy: 0.0833\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 4.0206 - accuracy: 0.0729\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 3.9926 - accuracy: 0.0729\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 3.9206 - accuracy: 0.1042\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 3.8871 - accuracy: 0.0833\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 3.8409 - accuracy: 0.0729\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 3.7865 - accuracy: 0.1146\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 3.7139 - accuracy: 0.1146\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 3.6572 - accuracy: 0.1146\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 3.5793 - accuracy: 0.1250\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 3.5047 - accuracy: 0.1354\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 3.4436 - accuracy: 0.1146\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 3.3816 - accuracy: 0.1250\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 3.3031 - accuracy: 0.1250\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 3.2415 - accuracy: 0.1146\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 3.1699 - accuracy: 0.1146\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 3.1034 - accuracy: 0.1146\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 3.0658 - accuracy: 0.1146\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 2.9781 - accuracy: 0.1354\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 2.9248 - accuracy: 0.1354\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 2.8588 - accuracy: 0.1458\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 2.7978 - accuracy: 0.1667\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 2.7345 - accuracy: 0.1667\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 2.6854 - accuracy: 0.1771\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 2.6379 - accuracy: 0.1667\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 2.5968 - accuracy: 0.1979\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 2.5302 - accuracy: 0.2396\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 2.4868 - accuracy: 0.2500\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 2.4424 - accuracy: 0.2708\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 2.4208 - accuracy: 0.2500\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 2.3663 - accuracy: 0.2917\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 2.3444 - accuracy: 0.2708\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 2.2841 - accuracy: 0.2812\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 2.2593 - accuracy: 0.3125\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 2.2308 - accuracy: 0.3125\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 2.1881 - accuracy: 0.3750\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 2.1671 - accuracy: 0.4062\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 2.1104 - accuracy: 0.4688\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 2.1025 - accuracy: 0.4375\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 2.0863 - accuracy: 0.3646\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 2.0732 - accuracy: 0.4167\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 2.0454 - accuracy: 0.4688\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 2.0114 - accuracy: 0.4896\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 1.9709 - accuracy: 0.4896\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 1.9457 - accuracy: 0.5417\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 1.9363 - accuracy: 0.4792\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 1.9040 - accuracy: 0.4896\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 1.8716 - accuracy: 0.5729\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 1.8502 - accuracy: 0.5104\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 1.8275 - accuracy: 0.5729\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 1.8005 - accuracy: 0.5833\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 1.7820 - accuracy: 0.6042\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.7625 - accuracy: 0.6146\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 1.7404 - accuracy: 0.6562\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 1.7175 - accuracy: 0.6771\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 1.6977 - accuracy: 0.6771\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.6834 - accuracy: 0.7083\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 1.6621 - accuracy: 0.7188\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 1.6371 - accuracy: 0.7292\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 1.6236 - accuracy: 0.7396\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 1.6096 - accuracy: 0.6771\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 1.5858 - accuracy: 0.7500\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 1.5707 - accuracy: 0.7396\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 1.5551 - accuracy: 0.7500\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 1.5457 - accuracy: 0.7292\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 1.5276 - accuracy: 0.7292\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.5089 - accuracy: 0.7708\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.5341 - accuracy: 0.7500\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 1.5462 - accuracy: 0.7083\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 1.5061 - accuracy: 0.7292\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 1.4903 - accuracy: 0.7292\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 1.4824 - accuracy: 0.7604\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 1.4563 - accuracy: 0.7500\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 1.4261 - accuracy: 0.8438\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 1.4098 - accuracy: 0.8333\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 1.3867 - accuracy: 0.8438\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.3791 - accuracy: 0.8542\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 1.3629 - accuracy: 0.8542\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 1.3574 - accuracy: 0.8542\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 1.3346 - accuracy: 0.8542\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.3185 - accuracy: 0.8646\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 1.3123 - accuracy: 0.8750\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 1.2957 - accuracy: 0.8958\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 1.3233 - accuracy: 0.8542\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 1.3491 - accuracy: 0.8646\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 1.3579 - accuracy: 0.7917\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 1.2976 - accuracy: 0.8125\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 1.2871 - accuracy: 0.8438\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 1.2899 - accuracy: 0.8542\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 1.2473 - accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d17ec426-0007-4698-ab7a-87e6d397fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer):\n",
    "    for _ in range(next_words):\n",
    "        # print(tokenizer.texts_to_sequences([seed_text]))\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        # print(\"predicted_probs: \", predicted_probs)\n",
    "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d523d640-f9e8-435a-874f-c776ecc4853e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Once upon a a time a a village there lived a curious girl\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Once upon a\"\n",
    "next_words = 10\n",
    "generated = generate_text(seed_text, next_words, model, max_seq_len, tokenizer)\n",
    "print(\"Generated text:\\n\", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5979661c-e5ba-46ce-ba65-3dadaf1c6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"text_gen_lstm_model.h5\")\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d03215f9-840a-439d-8541-3fefd42a775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:52:01.868319: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-06-01 10:52:01.869487: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-06-01 10:52:01.870447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"text_gen_lstm_model.h5\")\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b05280-87e6-4a4b-b394-23719ef9df5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
