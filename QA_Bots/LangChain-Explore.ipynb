{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7060aac6-4a57-4b90-8897-152cb2542164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Make sure to explain it in a way that is easy to understand for beginners, and provide examples of how it is being used in different industries. Use clear and concise language, and avoid any technical jargon or complex terminology. Additionally, include visuals such as images or videos to help illustrate your points. Lastly, provide a call-to-action at the end of your blog, encouraging readers to learn more about Generative AI and its potential impact on the future of technology.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint  # ✅ NEW package\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load token\n",
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# Use hosted HuggingFace model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",  # ✅ Replace with your preferred model\n",
    "    temperature=0.7, \n",
    "    max_new_tokens=256  # ✅ Use `model_kwargs`\n",
    ")\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate.from_template(\"Write a short and simple blog about {topic}.\")\n",
    "\n",
    "# Combine using modern pipe (`|`) syntax\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\"topic\": \"What is Generative AI\"})\n",
    "\n",
    "# Print output\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "173c00db-198b-4324-bba7-822b11b5a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install dotenv\n",
    "# !pip install pypdf\n",
    "# !pip install langchain_community\n",
    "# !pip install sentence-transformers\n",
    "# !pip install -U langchain-huggingface\n",
    "# !pip install faiss-cpu\n",
    "# !pip install sentence-transformers\n",
    "# !pip install flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02134d5d-81e4-458c-85fe-3c50eb7852bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d0c008aab144c7b4c4ad86280be6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0c40b1da764dea9b6ef9bef6068cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d68c7712cb43289b5040700ef915f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866b2ed21e004111891c679868e854e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41d9f8bdca74d88bc5b64603cc33e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a719a04e5f440c2ba0988b98535fdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323649d15959477eae599a715415af40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e569d2e5ed294d18aacb72a53fb4bb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56320e5eaf774b72bf80f0410b6fb0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dfcb145c60420f8177c4e49de0467d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e16b78bc3ed4e20bf80a5e991f993fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  What is RAG and how does it works?\n",
      "A:   RAG (Retrieval-Augmented Generation) is a technique that combines a language model with a retrieval mechanism. When a user asks a question, the system first retrieves relevant documents, then passes them to the LLM (Large Language Model) to generate a more accurate answer. RAG improves factual accuracy, makes the model more reliable, and enables domain-specific responses. This technique is especially useful in domains where accuracy and real-time data matter. RAG helps automate and augment work in various domains like education, marketing, and software development. However, LLMs (Large Language Models) have limitations, such as hallucinating, lacking up-to-date knowledge, and being sensitive to prompt phrasing, which limits their use in these areas.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the environment\n",
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# Step 1. Load the pdf\n",
    "loader = PyPDFLoader(\"Generative_AI_RAG_Intro.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# Step 2 Split the text\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(pages)\n",
    "# print(docs)\n",
    "\n",
    "# Step 3: Embed text\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Step 4: Setup Huggingface LLM\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "# Step 5: Retrieval QA Chain\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever(), return_source_documents=False)\n",
    "\"\"\"\n",
    "So when a question is asked:\n",
    "    It is converted into an embedding.\n",
    "    FAISS finds similar embedded chunks.\n",
    "    Those chunks are passed to the LLM to form an answer\n",
    "\"\"\"\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is RAG and how does it works?\"\n",
    "response = qa.invoke(query)\n",
    "\n",
    "print(\"Q: \", query)\n",
    "print(\"A: \", response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d554a756-aa3b-4b51-96b9-036b2b839e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the capital of india?\"\n",
    "# response = qa.invoke(query)\n",
    "# print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124e19b-bc00-4dba-86e7-d6636e07ac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34994a1-f1a1-48b7-879b-f26985598d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89862729-f73a-4678-ab07-a22ed1c83e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
