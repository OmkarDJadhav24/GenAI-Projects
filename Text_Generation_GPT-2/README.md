# GPT-2 Fine-Tuned Text Generation API

This project showcases a complete **GenAI pipeline** where we:

- Fine-tuned OpenAIâ€™s GPT-2 on the [Tiny Shakespeare dataset](https://huggingface.co/datasets/tiny_shakespeare)
- Exposed it via a robust, production-style **FastAPI** backend
- Enabled text generation through a REST API

---

## Model

- **Base Model**: `gpt2` from Hugging Face Transformers
- **Training Dataset**: Tiny Shakespeare
- **Fine-Tuning Framework**: Hugging Face `Trainer` API
- **Tokenization**: GPT-2 tokenizer with `eos_token` as padding token

---

## ðŸ“‚ Project Structure

Text_Generation_GPT-2

â”œâ”€â”€ app/

â”‚ â”œâ”€â”€ main.py # FastAPI app entrypoint

â”‚ â”œâ”€â”€ routes/

â”‚ â”‚ â””â”€â”€ genai.py # API routes

â”‚ â”œâ”€â”€ models/

â”‚ â”‚ â””â”€â”€ request.py # Request schema

â”‚ â””â”€â”€ utils/

â”‚ â””â”€â”€ inference.py # GPT-2 inference logic

â”œâ”€â”€ gpt2-finetuned/ # Saved fine-tuned model

â”œâ”€â”€ text_generation_gpt2.ipynb # Script for fine-tuning GPT-2

â””â”€â”€ README.md